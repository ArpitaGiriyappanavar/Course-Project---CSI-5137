{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Bayesian_optimization-NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4z_1znWpKQ2"
      },
      "source": [
        "# **Install bayesian-optimization Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "462ptJO1oUpl",
        "outputId": "037fdfe9-feed-47a2-a1cc-af0d70ce660c"
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7a/fd8059a3881d3ab37ac8f72f56b73937a14e8bb14a9733e68cc8b17dbe3c/bayesian-optimization-1.2.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.19.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-cp36-none-any.whl size=11685 sha256=b7b3ec46e51094fad44dfa933dda2c6588dff146380458ee3f2d8d36ee86ed26\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/56/ae/e0e3c1fc1954dc3ec712e2df547235ed072b448094d8f94aec\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CczOGu4pa7e"
      },
      "source": [
        "# **Install gpy gpyopt Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwsC4rqHqbNu",
        "outputId": "3f3c45ed-1e4b-4e6b-a968-51b272778a10"
      },
      "source": [
        "pip install gpy gpyopt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/976598f98adbfa918a480cb2d643f93fb555ca5b6c5614f76b69678114c1/GPy-1.9.9.tar.gz (995kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.7MB/s \n",
            "\u001b[?25hCollecting gpyopt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/be/669d505416d7e465b2aef7df3b58d590f56468c4f7dc50c91fe91b8a78d9/GPyOpt-1.2.6.tar.gz (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from gpy) (1.19.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from gpy) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gpy) (1.15.0)\n",
            "Collecting paramz>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/37/4abbeb78d30f20d3402887f46e6e9f3ef32034a9dea65d243654c82c8553/paramz-0.9.5.tar.gz (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.6/dist-packages (from paramz>=0.9.0->gpy) (4.4.2)\n",
            "Building wheels for collected packages: gpy, gpyopt, paramz\n",
            "  Building wheel for gpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpy: filename=GPy-1.9.9-cp36-cp36m-linux_x86_64.whl size=2633933 sha256=514230514d2704bbec53ab470f74c29882c93f38bbb1357ac2fb417e36b7de55\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/36/66/2b58860c84c9f2b51615da66bfd6feeddbc4e04d887ff96dfa\n",
            "  Building wheel for gpyopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpyopt: filename=GPyOpt-1.2.6-cp36-none-any.whl size=83623 sha256=d04c9d5547892e691cdfd80fd874780ed4c22cccea1ceb7503415b9920af72c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/00/69/cfa967a125cf25e66f644be6193ad6f0edf231147879ad714f\n",
            "  Building wheel for paramz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paramz: filename=paramz-0.9.5-cp36-none-any.whl size=102552 sha256=6a80cb9a89892618ff6bffa96661ecb802240a4dbe35bad713d255310a316cbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/4a/0e/6e0dc85541825f991c431619e25b870d4b812c911214690cf8\n",
            "Successfully built gpy gpyopt paramz\n",
            "Installing collected packages: paramz, gpy, gpyopt\n",
            "Successfully installed gpy-1.9.9 gpyopt-1.2.6 paramz-0.9.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8XclE-tpiSZ"
      },
      "source": [
        "# **Import necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liupXiovqRqP"
      },
      "source": [
        "import GPy, GPyOpt\n",
        "import numpy as np\n",
        "import pandas as pds\n",
        "import random\n",
        "from keras.layers import Activation, Dropout, BatchNormalization, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.datasets import mnist\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxbIF4DbpqKm"
      },
      "source": [
        "# **MNIST class -  functions for data loading, training, fitness calculation and evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mhlU7xQqr3r"
      },
      "source": [
        "# MNIST class\n",
        "class MNIST():\n",
        "    def __init__(self,\n",
        "                 l1_out=512, \n",
        "                 l2_out=512, \n",
        "                 l1_drop=0.2, \n",
        "                 l2_drop=0.2, \n",
        "                 bn1=0,\n",
        "                 bn2=0,\n",
        "                 batch_size=100, \n",
        "                 epochs=10, \n",
        "                 validation_split=0.1):\n",
        "        self.l1_out = l1_out\n",
        "        self.l2_out = l2_out\n",
        "        self.l1_drop = l1_drop\n",
        "        self.l2_drop = l2_drop\n",
        "        self.bn1 = bn1\n",
        "        self.bn2 = bn2\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.validation_split = validation_split\n",
        "        self.__x_train, self.__x_test, self.__y_train, self.__y_test = self.mnist_data()\n",
        "        self.__model = self.mnist_model()\n",
        "        \n",
        "    # load mnist data from keras dataset\n",
        "    def mnist_data(self):\n",
        "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "        X_train = X_train.reshape(60000, 784)\n",
        "        X_test = X_test.reshape(10000, 784)\n",
        "\n",
        "        X_train = X_train.astype('float32')\n",
        "        X_test = X_test.astype('float32')\n",
        "        X_train /= 255\n",
        "        X_test /= 255\n",
        "\n",
        "        Y_train = np_utils.to_categorical(y_train, 10)\n",
        "        Y_test = np_utils.to_categorical(y_test, 10)\n",
        "        return X_train, X_test, Y_train, Y_test\n",
        "    \n",
        "    # mnist model\n",
        "    def mnist_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(self.l1_out, input_shape=(784,)))\n",
        "        if self.bn1 == 0:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(self.l1_drop))\n",
        "        model.add(Dense(self.l2_out))\n",
        "        if self.bn2 == 0:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(self.l2_drop))\n",
        "        model.add(Dense(10))\n",
        "        model.add(Activation('softmax'))\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "    \n",
        "    # fit mnist model\n",
        "    def mnist_fit(self):\n",
        "        early_stopping = EarlyStopping(patience=0, verbose=1)\n",
        "        \n",
        "        self.__model.fit(self.__x_train, self.__y_train,\n",
        "                       batch_size=self.batch_size,\n",
        "                       epochs=self.epochs,\n",
        "                       verbose=0,\n",
        "                       validation_split=self.validation_split,\n",
        "                       callbacks=[early_stopping])\n",
        "    \n",
        "    # evaluate mnist model\n",
        "    def evaluate_mnist(self):\n",
        "        self.mnist_fit()\n",
        "        \n",
        "        evaluation = self.__model.evaluate(self.__x_test, self.__y_test, batch_size=self.batch_size, verbose=0)\n",
        "        return evaluation"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfRISQjFqpp7"
      },
      "source": [
        "# **Run the model for MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnBWjWbhqx4T"
      },
      "source": [
        "\n",
        "# function to run mnist class\n",
        "def run_mnist(l1_out=512, l2_out=512, \n",
        "              l1_drop=0.2, l2_drop=0.2, \n",
        "              bn1=0, bn2=0,\n",
        "              batch_size=100, epochs=10, validation_split=0.1):\n",
        "    \n",
        "    _mnist = MNIST()\n",
        "    mnist_evaluation = _mnist.evaluate_mnist()\n",
        "    return mnist_evaluation"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F0U6GsNq0fG"
      },
      "source": [
        "# **Obtaining the bounds for hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDqC0WKrq8ya"
      },
      "source": [
        "# bounds for hyper-parameters in mnist model\n",
        "# the bounds dict should be in order of continuous type and then discrete type\n",
        "bounds = [{'name': 'validation_split', 'type': 'continuous',  'domain': (0.0, 0.3)},\n",
        "          {'name': 'l1_drop',          'type': 'continuous',  'domain': (0.0, 0.3)},\n",
        "          {'name': 'l2_drop',          'type': 'continuous',  'domain': (0.0, 0.3)},\n",
        "          {'name': 'l1_out',           'type': 'discrete',    'domain': (64, 128, 256, 512, 1024)},\n",
        "          {'name': 'l2_out',           'type': 'discrete',    'domain': (64, 128, 256, 512, 1024)},\n",
        "          {'name': 'bn1',              'type': 'discrete',    'domain': (0, 1)},\n",
        "          {'name': 'bn2',              'type': 'discrete',    'domain': (0, 1)},\n",
        "          {'name': 'batch_size',       'type': 'discrete',    'domain': (10, 100, 500)},\n",
        "          {'name': 'epochs',           'type': 'discrete',    'domain': (5, 10, 20)}]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMLiTmQVq_xg"
      },
      "source": [
        "# **Bayesian Optimization Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNwgWBOtrBAg"
      },
      "source": [
        "# function to optimize mnist model\n",
        "def f(x):\n",
        "    print(x)\n",
        "    evaluation = run_mnist(\n",
        "        l1_drop = int(x[:,1]), \n",
        "        l2_drop = int(x[:,2]), \n",
        "        l1_out = float(x[:,3]),\n",
        "        l2_out = float(x[:,4]), \n",
        "        bn1 = int(x[:,5]),\n",
        "        bn2 = int(x[:,6]),\n",
        "        batch_size = int(x[:,7]), \n",
        "        epochs = int(x[:,8]), \n",
        "        validation_split = float(x[:,0]))\n",
        "    print(\"loss:{0} \\t\\t accuracy:{1}\".format(evaluation[0], evaluation[1]))\n",
        "    return evaluation[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXIVUt4RrIVo"
      },
      "source": [
        "# **Instance for Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbMtqstErHIH",
        "outputId": "69447f40-2bed-408f-ccf9-2ee5085cb422"
      },
      "source": [
        "# optimizer\n",
        "opt_mnist = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.17182991e-01 1.32565514e-01 2.86688817e-01 5.12000000e+02\n",
            "  1.28000000e+02 1.00000000e+00 0.00000000e+00 5.00000000e+02\n",
            "  1.00000000e+01]]\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 00005: early stopping\n",
            "loss:0.06382651627063751 \t\t accuracy:0.9793999791145325\n",
            "[[2.56116831e-02 1.30016176e-01 1.50512844e-01 1.28000000e+02\n",
            "  5.12000000e+02 0.00000000e+00 0.00000000e+00 5.00000000e+02\n",
            "  2.00000000e+01]]\n",
            "Epoch 00006: early stopping\n",
            "loss:0.07548926025629044 \t\t accuracy:0.9765999913215637\n",
            "[[2.74490436e-01 1.16512479e-01 2.75954016e-01 1.28000000e+02\n",
            "  1.28000000e+02 0.00000000e+00 1.00000000e+00 1.00000000e+02\n",
            "  5.00000000e+00]]\n",
            "Epoch 00008: early stopping\n",
            "loss:0.06443511694669724 \t\t accuracy:0.9814000129699707\n",
            "[[1.86705664e-01 2.06677203e-01 1.08750486e-01 1.28000000e+02\n",
            "  5.12000000e+02 1.00000000e+00 1.00000000e+00 1.00000000e+02\n",
            "  5.00000000e+00]]\n",
            "Epoch 00005: early stopping\n",
            "loss:0.07538910955190659 \t\t accuracy:0.9778000116348267\n",
            "[[2.26998270e-02 1.39335094e-01 1.26445048e-01 6.40000000e+01\n",
            "  1.28000000e+02 0.00000000e+00 0.00000000e+00 5.00000000e+02\n",
            "  1.00000000e+01]]\n",
            "Epoch 00004: early stopping\n",
            "loss:0.06211815029382706 \t\t accuracy:0.98089998960495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKNrb6BMro-Q"
      },
      "source": [
        "# **Run the optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMYjoNRSrLSb",
        "outputId": "d9a7ef41-80fc-4d68-aea5-1b30b19020ec"
      },
      "source": [
        "# optimize mnist model\n",
        "opt_mnist.run_optimization(max_iter=15)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.08845670e-01 1.02703193e-01 9.02670597e-02 6.40000000e+01\n",
            "  1.28000000e+02 0.00000000e+00 1.00000000e+00 5.00000000e+02\n",
            "  1.00000000e+01]]\n",
            "Epoch 00005: early stopping\n",
            "loss:0.0743269994854927 \t\t accuracy:0.9776999950408936\n",
            "[[8.60897233e-03 1.07679352e-01 7.40544129e-02 6.40000000e+01\n",
            "  1.28000000e+02 0.00000000e+00 0.00000000e+00 5.00000000e+02\n",
            "  1.00000000e+01]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.08681721985340118 \t\t accuracy:0.9735999703407288\n",
            "[[3.88434817e-02 1.16884732e-01 2.83535178e-01 5.12000000e+02\n",
            "  1.28000000e+02 1.00000000e+00 0.00000000e+00 5.00000000e+02\n",
            "  1.00000000e+01]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.0843592956662178 \t\t accuracy:0.9740999937057495\n",
            "[[1.21474907e-01 5.25444509e-02 2.16087269e-02 1.28000000e+02\n",
            "  1.28000000e+02 1.00000000e+00 1.00000000e+00 1.00000000e+02\n",
            "  5.00000000e+00]]\n",
            "Epoch 00004: early stopping\n",
            "loss:0.07485908269882202 \t\t accuracy:0.9783999919891357\n",
            "[[  0.17234373   0.18179626   0.16872517  64.          64.\n",
            "    1.           0.         100.           5.        ]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.08463800698518753 \t\t accuracy:0.9735999703407288\n",
            "[[4.05522402e-02 7.24819611e-02 9.17738051e-02 1.28000000e+02\n",
            "  1.28000000e+02 0.00000000e+00 1.00000000e+00 1.00000000e+02\n",
            "  5.00000000e+00]]\n",
            "Epoch 00004: early stopping\n",
            "loss:0.06735677272081375 \t\t accuracy:0.9796000123023987\n",
            "[[2.43347812e-01 1.09011991e-01 9.20119508e-02 1.28000000e+02\n",
            "  1.28000000e+02 0.00000000e+00 1.00000000e+00 1.00000000e+02\n",
            "  5.00000000e+00]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.08642011135816574 \t\t accuracy:0.973800003528595\n",
            "[[5.95488838e-02 1.76180338e-01 1.21089825e-01 5.12000000e+02\n",
            "  1.28000000e+02 1.00000000e+00 0.00000000e+00 5.00000000e+02\n",
            "  1.00000000e+01]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.10514382272958755 \t\t accuracy:0.9672999978065491\n",
            "[[1.27454792e-01 1.17224278e-01 2.97337704e-03 1.28000000e+02\n",
            "  1.28000000e+02 0.00000000e+00 1.00000000e+00 1.00000000e+02\n",
            "  5.00000000e+00]]\n",
            "Epoch 00004: early stopping\n",
            "loss:0.08813740313053131 \t\t accuracy:0.9733999967575073\n",
            "[[1.11901262e-01 1.20320015e-01 1.93083494e-01 1.28000000e+02\n",
            "  1.28000000e+02 0.00000000e+00 1.00000000e+00 1.00000000e+02\n",
            "  5.00000000e+00]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.07865802943706512 \t\t accuracy:0.9753000140190125\n",
            "[[3.06068252e-02 1.68884668e-01 2.30673899e-02 2.56000000e+02\n",
            "  1.28000000e+02 0.00000000e+00 0.00000000e+00 1.00000000e+01\n",
            "  1.00000000e+01]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.102301687002182 \t\t accuracy:0.9700000286102295\n",
            "[[2.48774945e-01 1.17395432e-01 1.02564639e-01 2.56000000e+02\n",
            "  1.28000000e+02 1.00000000e+00 0.00000000e+00 1.00000000e+01\n",
            "  2.00000000e+01]]\n",
            "Epoch 00004: early stopping\n",
            "loss:0.06828107684850693 \t\t accuracy:0.979200005531311\n",
            "[[1.16077369e-01 1.24607923e-01 2.45310307e-01 1.02400000e+03\n",
            "  2.56000000e+02 0.00000000e+00 0.00000000e+00 1.00000000e+02\n",
            "  2.00000000e+01]]\n",
            "Epoch 00003: early stopping\n",
            "loss:0.07855638116598129 \t\t accuracy:0.9745000004768372\n",
            "[[1.47610185e-01 2.85229806e-01 1.78335121e-01 5.12000000e+02\n",
            "  2.56000000e+02 0.00000000e+00 1.00000000e+00 5.00000000e+02\n",
            "  1.00000000e+01]]\n",
            "Epoch 00004: early stopping\n",
            "loss:0.07981560379266739 \t\t accuracy:0.9735999703407288\n",
            "[[1.70842814e-01 2.89808318e-02 2.86832045e-01 1.28000000e+02\n",
            "  2.56000000e+02 0.00000000e+00 0.00000000e+00 1.00000000e+02\n",
            "  2.00000000e+01]]\n",
            "Epoch 00004: early stopping\n",
            "loss:0.07281825691461563 \t\t accuracy:0.977400004863739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9imxCjQ_rtqJ"
      },
      "source": [
        "# **Output Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3egqYM-rOEK"
      },
      "source": [
        "optbounds = {\n",
        "    'validation_split' : opt_mnist.x_opt[0],\n",
        "    'l1_drop' : opt_mnist.x_opt[1],\n",
        "    'l2_drop' : opt_mnist.x_opt[2],\n",
        "    'l1_out' : opt_mnist.x_opt[3],\n",
        "    'l2_out' : opt_mnist.x_opt[4],\n",
        "    'bn1' : opt_mnist.x_opt[5],\n",
        "    'bn2' : opt_mnist.x_opt[6],\n",
        "    'batch_size' : opt_mnist.x_opt[7],\n",
        "    'epochs' : opt_mnist.x_opt[8],\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssqieITIrzux"
      },
      "source": [
        "# **The optimized hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXfWqbmFrRZD",
        "outputId": "8ebb1cf4-730d-407b-9c34-b40bc65ba423"
      },
      "source": [
        "# print optimized mnist model\n",
        "print(\"optimized parameters: {0}\".format(optbounds))\n",
        "print(\"optimized loss: {0}\".format(opt_mnist.fx_opt))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimized parameters: {'validation_split': 0.022699826989121173, 'l1_drop': 0.1393350944111121, 'l2_drop': 0.12644504812304083, 'l1_out': 64.0, 'l2_out': 128.0, 'bn1': 0.0, 'bn2': 0.0, 'batch_size': 500.0, 'epochs': 10.0}\n",
            "optimized loss: 0.06211815029382706\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}